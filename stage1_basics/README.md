### **1ï¸âƒ£ Imports and Setup**

```python
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
from tqdm import tqdm
```

* `torch` â†’ main PyTorch library âš¡
* `nn` â†’ for building neural networks ğŸ—ï¸
* `optim` â†’ for optimization (like adjusting weights) ğŸ› ï¸
* `DataLoader` â†’ helps feed data into your model in batches ğŸ½ï¸
* `datasets` & `transforms` â†’ get datasets & preprocess images ğŸ–¼ï¸
* `models` â†’ prebuilt models like Vision Transformer (ViT) ğŸ¤–
* `tqdm` â†’ makes a progress bar for loops â³

---

### **2ï¸âƒ£ Device Selection**

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

* Checks if you have a GPU (CUDA). If yes â†’ use GPU âš¡, else CPU ğŸ¢.
* GPU = much faster training for deep learning ğŸ’¨

---

### **3ï¸âƒ£ Image Transformations**

```python
transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
```

* `Resize(224)` â†’ ViT wants 224x224 images ğŸ“
* `ToTensor()` â†’ converts images to PyTorch-friendly format ğŸ”¢
* `Normalize()` â†’ scales pixel values so the model learns better ğŸšï¸

---

### **4ï¸âƒ£ Load CIFAR-10**

```python
train_set = datasets.CIFAR10(root="./data", train=True, download=True, transform=transform)
test_set = datasets.CIFAR10(root="./data", train=False, download=True, transform=transform)

train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
test_loader = DataLoader(test_set, batch_size=32, shuffle=False)
```

* CIFAR-10 â†’ 60k tiny images in 10 classes (cats, planes, etc.) ğŸ±âœˆï¸
* `DataLoader` â†’ feeds batches of 32 images at a time ğŸ”„
* `shuffle=True` â†’ randomizes training data for better learning ğŸ²

---

### **5ï¸âƒ£ Load Pretrained Vision Transformer**

```python
model = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)
num_features = model.heads.head.in_features
model.heads.head = nn.Linear(num_features, 10)
model = model.to(device)
```

* `vit_b_16` â†’ Vision Transformer, a fancy image model ğŸ–¼ï¸ğŸ¤–
* We replace the last layer with `nn.Linear(..., 10)` because CIFAR-10 has 10 classes ğŸ”„
* `.to(device)` â†’ moves the model to GPU if available ğŸ–¥ï¸

---

### **6ï¸âƒ£ Loss & Optimizer**

```python
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=3e-5)
```

* `CrossEntropyLoss()` â†’ standard for classification (e.g., cat vs dog) ğŸ·ï¸
* `Adam` â†’ smart way to update model weights ğŸ’ª
* `lr=3e-5` â†’ learning rate = how fast the model learns ğŸš€

---

### **7ï¸âƒ£ Training Loop**

```python
epochs = 3
for epoch in range(epochs):
    model.train()
    total_loss = 0

    loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", ncols=100)
    for images, labels in loop:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loop.set_postfix(loss=loss.item())
```

* Loop over epochs (full passes through dataset) ğŸ”
* `model.train()` â†’ tells PyTorch we're training âš™ï¸
* Forward pass: `outputs = model(images)` â†’ model guesses ğŸ§ 
* Compute loss: `loss = criterion(...)` â†’ how wrong the guesses were âŒ
* Backprop: `loss.backward()` â†’ calculate gradients ğŸ”„
* Update weights: `optimizer.step()` â†’ model learns ğŸ“ˆ
* `tqdm` shows progress bar â³

---

### **8ï¸âƒ£ Evaluation**

```python
model.eval()
correct, total = 0, 0

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
```

* `model.eval()` â†’ turn off training stuff like dropout ğŸ“´
* `torch.no_grad()` â†’ saves memory during evaluation ğŸ’¾
* Compare predictions vs labels to get accuracy âœ…

---

### **9ï¸âƒ£ Save the Model**

```python
Path("models").mkdir(exist_ok=True, parents=True)
torch.save(model.state_dict(), "models/vit_cifar10.pth")
```

* Create a `models/` folder if it doesn't exist ğŸ“‚
* Save model weights so you can load later ğŸ’¾

---

### **Summary in One Line**

1. Load CIFAR-10 images ğŸ–¼ï¸
2. Preprocess them (resize, normalize) ğŸ¨
3. Load a pretrained Vision Transformer ğŸ¤–
4. Replace last layer for CIFAR-10 classes ğŸ”„
5. Train for 3 epochs âš¡
6. Check accuracy âœ…
7. Save the trained model ğŸ’¾

<br>
